{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ae4966",
   "metadata": {},
   "source": [
    "# Assignment 02: Crime classification system\n",
    "\n",
    "In this Assignment, we will predict the crime category using the description. Some classification algorithms need to use: \n",
    "\n",
    "- LogisticRegression\n",
    "- Random Forest\n",
    "\n",
    "Besides, we will use some `tools for NLP`, including: \n",
    "\n",
    "- Tokenization\n",
    "- StopWordsRemover\n",
    "- CountVectorizer\n",
    "- StringIndexer\n",
    "\n",
    "To make the code clear, we are going to split the code into some sections based on the Evaluation Criteria below:\n",
    "\n",
    "- **Section 1**: Load data source and show the most categories of crime and descriptions of crime\n",
    "\n",
    "- **Section 2**: Build a data pipeline using Tokenization, Remove Stop Words and Count vectors. Train the model by using the LogisticRegression Algorithm\n",
    "\n",
    "- **Section 3**: Modify the pipeline by adding TF-IDF. Training the model by Logistic Algorithm\n",
    "\n",
    "- **Section 4**: Re-train the model by Random Forest\n",
    "\n",
    "- **Section 5**: Applying CrossValidation for trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a806c84",
   "metadata": {},
   "source": [
    "### Section 1: Load data source and show the most categories of crime and descriptions of crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5da180e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T03:34:47.269918900Z",
     "start_time": "2023-09-22T03:34:47.266321900Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77f502c",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-22T03:34:55.005726100Z",
     "start_time": "2023-09-22T03:34:48.572995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dates: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      "\n",
      "+---------------+--------------+--------------------+---------+----------+--------------+--------------------+------------+-----------+\n",
      "|          Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|           X|          Y|\n",
      "+---------------+--------------+--------------------+---------+----------+--------------+--------------------+------------+-----------+\n",
      "|5/13/2015 23:53|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|-122.4258917| 37.7745986|\n",
      "|5/13/2015 23:53|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|-122.4258917| 37.7745986|\n",
      "|5/13/2015 23:33|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...| -122.424363|37.80041432|\n",
      "|5/13/2015 23:30| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-122.4269953|37.80087263|\n",
      "|5/13/2015 23:30| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|-122.4387376|37.77154117|\n",
      "|5/13/2015 23:30| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday| INGLESIDE|          NONE| 0 Block of TEDDY AV|-122.4032524| 37.7134307|\n",
      "|5/13/2015 23:30| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday| INGLESIDE|          NONE| AVALON AV / PERU AV| -122.423327|37.72513804|\n",
      "|5/13/2015 23:30| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday|   BAYVIEW|          NONE|KIRKWOOD AV / DON...|-122.3712743|37.72756407|\n",
      "|5/13/2015 23:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  RICHMOND|          NONE|600 Block of 47TH AV| -122.508194|37.77660126|\n",
      "|5/13/2015 23:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|-122.4190877|37.80780155|\n",
      "|5/13/2015 22:58| LARCENY/THEFT|PETTY THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|-122.4190877|37.80780155|\n",
      "|5/13/2015 22:30|OTHER OFFENSES|MISCELLANEOUS INV...|Wednesday|   TARAVAL|          NONE|0 Block of ESCOLT...|-122.4879831|37.73766665|\n",
      "|5/13/2015 22:30|     VANDALISM|MALICIOUS MISCHIE...|Wednesday|TENDERLOIN|          NONE|  TURK ST / JONES ST|-122.4124143| 37.7830038|\n",
      "|5/13/2015 22:06| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|FILLMORE ST / GEA...|-122.4329146|37.78435334|\n",
      "|5/13/2015 22:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|   BAYVIEW|          NONE|200 Block of WILL...|-122.3977444|37.72993469|\n",
      "|5/13/2015 22:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|   BAYVIEW|          NONE|0 Block of MENDEL...|-122.3836915|37.74318904|\n",
      "|5/13/2015 22:00|       ROBBERY|ROBBERY, ARMED WI...|Wednesday|TENDERLOIN|          NONE|  EDDY ST / JONES ST|-122.4125974|37.78393203|\n",
      "|5/13/2015 21:55|       ASSAULT|AGGRAVATED ASSAUL...|Wednesday| INGLESIDE|          NONE|GODEUS ST / MISSI...|-122.4216815| 37.7428222|\n",
      "|5/13/2015 21:40|OTHER OFFENSES|   TRAFFIC VIOLATION|Wednesday|   BAYVIEW|ARREST, BOOKED|MENDELL ST / HUDS...|-122.3864009|37.73898349|\n",
      "|5/13/2015 21:30|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|TENDERLOIN|          NONE|100 Block of JONE...|-122.4122498|37.78255633|\n",
      "+---------------+--------------+--------------------+---------+----------+--------------+--------------------+------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[6]') \\\n",
    "    .setAppName('ASM2')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "data = spark.read.csv('train.csv', inferSchema=True, header=True)\n",
    "data.printSchema()\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ad2293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            Category| count|\n",
      "+--------------------+------+\n",
      "|       LARCENY/THEFT|174900|\n",
      "|      OTHER OFFENSES|126182|\n",
      "|        NON-CRIMINAL| 92304|\n",
      "|             ASSAULT| 76876|\n",
      "|       DRUG/NARCOTIC| 53971|\n",
      "|       VEHICLE THEFT| 53781|\n",
      "|           VANDALISM| 44725|\n",
      "|            WARRANTS| 42214|\n",
      "|            BURGLARY| 36755|\n",
      "|      SUSPICIOUS OCC| 31414|\n",
      "|      MISSING PERSON| 25989|\n",
      "|             ROBBERY| 23000|\n",
      "|               FRAUD| 16679|\n",
      "|FORGERY/COUNTERFE...| 10609|\n",
      "|     SECONDARY CODES|  9985|\n",
      "|         WEAPON LAWS|  8555|\n",
      "|        PROSTITUTION|  7484|\n",
      "|            TRESPASS|  7326|\n",
      "|     STOLEN PROPERTY|  4540|\n",
      "|SEX OFFENSES FORC...|  4388|\n",
      "+--------------------+------+\n"
     ]
    }
   ],
   "source": [
    "# The list of 20 most occurrence categories of crime\n",
    "cat_df = data.groupBy(\"Category\").count()\n",
    "\n",
    "cat_df.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af4b562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Descript|count|\n",
      "+--------------------+-----+\n",
      "|GRAND THEFT FROM ...|60022|\n",
      "|       LOST PROPERTY|31729|\n",
      "|             BATTERY|27441|\n",
      "|   STOLEN AUTOMOBILE|26897|\n",
      "|DRIVERS LICENSE, ...|26839|\n",
      "|      WARRANT ARREST|23754|\n",
      "|SUSPICIOUS OCCURR...|21891|\n",
      "|AIDED CASE, MENTA...|21497|\n",
      "|PETTY THEFT FROM ...|19771|\n",
      "|MALICIOUS MISCHIE...|17789|\n",
      "|   TRAFFIC VIOLATION|16471|\n",
      "|PETTY THEFT OF PR...|16196|\n",
      "|MALICIOUS MISCHIE...|15957|\n",
      "|THREATS AGAINST LIFE|14716|\n",
      "|      FOUND PROPERTY|12146|\n",
      "|ENROUTE TO OUTSID...|11470|\n",
      "|GRAND THEFT OF PR...|11010|\n",
      "|POSSESSION OF NAR...|10050|\n",
      "|PETTY THEFT FROM ...|10029|\n",
      "|PETTY THEFT SHOPL...| 9571|\n",
      "+--------------------+-----+\n"
     ]
    }
   ],
   "source": [
    "# The list of 20 most commted descripts\n",
    "des_df = data.groupBy(\"Descript\").count()\n",
    "\n",
    "des_df.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b250756",
   "metadata": {},
   "source": [
    "### Section 2: Build a data pipeline using Tokenization, Remove Stop Words and Count vectors. Train the model by using the LogisticRegression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ea630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "\n",
    "# declare all items of the data pipeline\n",
    "category_to_num = StringIndexer(inputCol='Category',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"Descript\", outputCol=\"words\")\n",
    "stopremove = StopWordsRemover(inputCol=\"words\",outputCol=\"filtered\")\n",
    "count_vec = CountVectorizer(inputCol=\"filtered\",outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b90cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# apply the pipeline to the `data` dataframe\n",
    "data_prep_pipe = Pipeline(stages=[category_to_num, tokenizer, stopremove, count_vec])\n",
    "cleaner = data_prep_pipe.fit(data)\n",
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3647176d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  7.0|(1057,[11,23],[1....|\n",
      "|  1.0|(1057,[8,11,26],[...|\n",
      "|  1.0|(1057,[8,11,26],[...|\n",
      "|  0.0|(1057,[0,1,2,3],[...|\n",
      "|  0.0|(1057,[0,1,2,3],[...|\n",
      "|  0.0|(1057,[0,1,2,92],...|\n",
      "|  5.0|(1057,[7,18],[1.0...|\n",
      "|  5.0|(1057,[7,18],[1.0...|\n",
      "|  0.0|(1057,[0,1,2,3],[...|\n",
      "|  0.0|(1057,[0,1,2,3],[...|\n",
      "|  0.0|(1057,[0,2,3,5],[...|\n",
      "|  1.0|(1057,[56,75],[1....|\n",
      "|  6.0|(1057,[9,10,13,35...|\n",
      "|  0.0|(1057,[0,1,2,3],[...|\n",
      "|  2.0|(1057,[4,28],[1.0...|\n",
      "|  2.0|(1057,[4,28],[1.0...|\n",
      "| 11.0|(1057,[95,121,171...|\n",
      "|  3.0|(1057,[38,44,47,4...|\n",
      "|  1.0|(1057,[8,26],[1.0...|\n",
      "|  2.0|(1057,[4,28],[1.0...|\n",
      "+-----+--------------------+\n"
     ]
    }
   ],
   "source": [
    "clean_data = clean_data.select('label','features')\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573e1262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the clean data set to training and testing\n",
    "(training, testing) = clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b095aa2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "|  0.0|(1057,[0,1,2,3],[...|[27.7816752840856...|[0.99999999995220...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# train the model by using the LogisticRegression Algorithm\n",
    "lr = LogisticRegression(labelCol='label')\n",
    "lr_model = lr.fit(training)\n",
    "training_sum = lr_model.summary\n",
    "\n",
    "# use the trained model to transform the test data set\n",
    "test_results = lr_model.transform(testing)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2a78ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting crime category was: 99.7436%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# evaluate the test result by MulticlassClassificationEvaluator\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting crime category was: {:.4f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b792e51",
   "metadata": {},
   "source": [
    "### Section 3: Modify the pipeline by adding TF-IDF. Training the model by Logistic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91f0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# adding the `HashingTF` and `IDF` to the pipeline\n",
    "category_to_num = StringIndexer(inputCol='Category',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"Descript\", outputCol=\"words\")\n",
    "stopremove = StopWordsRemover(inputCol=\"words\",outputCol=\"filtered\")\n",
    "count_vec = CountVectorizer(inputCol=\"filtered\",outputCol=\"raw_features\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"hashed_features\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"hashed_features\", outputCol=\"idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c948044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform results to features column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "clean_up = VectorAssembler(inputCols=['raw_features', 'idf_features'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f55c275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline to transform data set\n",
    "data_prep_pipe = Pipeline(stages=[category_to_num, tokenizer, stopremove, count_vec, hashingTF, idf, clean_up])\n",
    "cleaner = data_prep_pipe.fit(data)\n",
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b0313b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  7.0|(11057,[11,23,781...|\n",
      "|  1.0|(11057,[8,11,26,1...|\n",
      "|  1.0|(11057,[8,11,26,1...|\n",
      "|  0.0|(11057,[0,1,2,3,1...|\n",
      "|  0.0|(11057,[0,1,2,3,1...|\n",
      "|  0.0|(11057,[0,1,2,92,...|\n",
      "|  5.0|(11057,[7,18,4859...|\n",
      "|  5.0|(11057,[7,18,4859...|\n",
      "|  0.0|(11057,[0,1,2,3,1...|\n",
      "|  0.0|(11057,[0,1,2,3,1...|\n",
      "|  0.0|(11057,[0,2,3,5,1...|\n",
      "|  1.0|(11057,[56,75,127...|\n",
      "|  6.0|(11057,[9,10,13,3...|\n",
      "|  0.0|(11057,[0,1,2,3,1...|\n",
      "|  2.0|(11057,[4,28,3790...|\n",
      "|  2.0|(11057,[4,28,3790...|\n",
      "| 11.0|(11057,[95,121,17...|\n",
      "|  3.0|(11057,[38,44,47,...|\n",
      "|  1.0|(11057,[8,26,1063...|\n",
      "|  2.0|(11057,[4,28,3790...|\n",
      "+-----+--------------------+\n"
     ]
    }
   ],
   "source": [
    "clean_data = clean_data.select('label','features')\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa94f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the clean data set to training and tesing df\n",
    "(training,testing) = clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f4f59fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "|  0.0|(11057,[0,1,2,3,1...|[25.4573847413503...|[0.99999999943989...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# using LogisticRegression to train the data with new features\n",
    "lr = LogisticRegression(labelCol='label')\n",
    "lr_model = lr.fit(training)\n",
    "\n",
    "test_results = lr_model.transform(testing)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b7550b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting crime category after using IDF was: 99.7446%\n"
     ]
    }
   ],
   "source": [
    "# re-evaluate the new model\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting crime category after using IDF was: {:.4f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c0e87",
   "metadata": {},
   "source": [
    "### Section 4: Re-train the model by Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07e4625e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting crime category after using RandomForestClassifier was: 52.4966%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# re-train the model by using Random Forest\n",
    "rfc = RandomForestClassifier(labelCol='label',featuresCol='features')\n",
    "rfc_model = rfc.fit(training)\n",
    "\n",
    "# evaluate the test results\n",
    "test_results = rfc_model.transform(testing)\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting crime category after using RandomForestClassifier was: {:.4f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d0584",
   "metadata": {},
   "source": [
    "### Section 5: Advanced: Applying CrossValidation for trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb5ba0",
   "metadata": {},
   "source": [
    "### Part 1: Use CrossValidation to tune the LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cec7ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mc:\\Users\\truon\\anaconda3\\lib\\importlib\\_bootstrap.py\u001B[0m in \u001B[0;36m_lock_unlock_module\u001B[1;34m(name)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.core' has no attribute 'numerictypes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_20276\\2871739420.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtuning\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mParamGridBuilder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCrossValidator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# Create the parameter grid to search over\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mparam_grid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mParamGridBuilder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0maddGrid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mregParam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\truon\\anaconda3\\lib\\site-packages\\pyspark\\ml\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[0mmachine\u001B[0m \u001B[0mlearning\u001B[0m \u001B[0mpipelines\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \"\"\"\n\u001B[1;32m---> 22\u001B[1;33m from pyspark.ml.base import (\n\u001B[0m\u001B[0;32m     23\u001B[0m     \u001B[0mEstimator\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[0mModel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\truon\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msince\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparam\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mP\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcommon\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0minherit_doc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m from pyspark.ml.param.shared import (\n",
      "\u001B[1;32mc:\\Users\\truon\\anaconda3\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     30\u001B[0m )\n\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_gateway\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mJavaObject\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\truon\\anaconda3\\lib\\site-packages\\numpy\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    198\u001B[0m     \u001B[1;31m# Numpy 1.20.0, 2020-10-19\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m     __deprecated_attrs__[\"typeDict\"] = (\n\u001B[1;32m--> 200\u001B[1;33m         \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumerictypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtypeDict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    201\u001B[0m         \u001B[1;34m\"`np.typeDict` is a deprecated alias for `np.sctypeDict`.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    202\u001B[0m     )\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'numpy.core' has no attribute 'numerictypes'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create the parameter grid to search over\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .build()\n",
    "\n",
    "# Create the cross-validator with the logistic regression model and parameter grid\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=acc_eval, numFolds=2)\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cv_model = cv.fit(training)\n",
    "\n",
    "# Make predictions on the testing data using the best model found by the cross-validator\n",
    "test_results = cv_model.transform(testing)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "\n",
    "print(\"Accuracy of model at predicting crime category after using CrossValidator was: {:.4f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c51208",
   "metadata": {},
   "source": [
    "### Part 2: Use CrossValidation to tune the RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f53e13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9db716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting crime category after tuning with CrossValidator was: 90.0944%\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search over\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rfc.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rfc.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .build()\n",
    "\n",
    "# Create the cross-validator with the random forest classifier and parameter grid\n",
    "cv = CrossValidator(estimator=rfc, estimatorParamMaps=param_grid, evaluator=acc_eval, numFolds=2)\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cv_model = cv.fit(training)\n",
    "\n",
    "# Make predictions on the testing data using the best model found by the cross-validator\n",
    "test_results = cv_model.transform(testing)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting crime category after tuning with CrossValidator was: {:.4f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8f4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849cb087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd4b08c000e2fb57376b32ea0ad8f6c98fb433e97a7da87f6e679f6ff06b0fbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
